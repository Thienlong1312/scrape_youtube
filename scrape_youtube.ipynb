{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_scrape():\n",
    "    # 'rank': video xếp hạng thứ mấy trong tìm kiếm với key_search\n",
    "    # 'title': Tiêu đề video\n",
    "    # 'date_public': Ngày công chiếu\n",
    "    # 'description': Mô tả video\n",
    "    # 'tags': Tag video\n",
    "    # 'views': Số lượng người xem\n",
    "    # 'likes',: Số lượt like\n",
    "    # 'dislikes': Số lượt dislike\n",
    "    # 'num_cmt': Số lượng comment\n",
    "    # 'duration': Thời lượng video\n",
    "    # 'video_url': url của video\n",
    "    # 'channel_name': Tên kênh\n",
    "    # 'subs': Số lượt sub\n",
    "    # 'link_top_re': link of top recommended videos\n",
    "    return pd.DataFrame(columns=['key_search', 'rank', 'title', 'date_public',\n",
    "                                 'description', 'tags', 'views', 'likes',\n",
    "                                 'dislikes', 'num_cmt', 'duration_s', 'video_url',\n",
    "                                 'channel_name', 'subs', 'link_top_re', 'tit_des_tags_re'])\n",
    "\n",
    "def create_dic_find_key():\n",
    "    # Các thuộc tính quan trọng dùng để tìm thông tin trên web\n",
    "    return {\n",
    "            'title_date_public':['yt-formatted-string',{'class':'style-scope ytd-video-primary-info-renderer'}],\n",
    "            'description':['yt-formatted-string',{'class':'content style-scope ytd-video-secondary-info-renderer'}],\n",
    "            'tags':['meta',{'name':'keywords'}],\n",
    "            'views':['span',{'class':'view-count style-scope ytd-video-view-count-renderer'}],\n",
    "            'likes_dislikes':['yt-formatted-string',{'class':'style-scope ytd-toggle-button-renderer style-text'},'aria-label'],\n",
    "            'num_cmt':['h2',{'id':'count','class':'style-scope ytd-comments-header-renderer'},\n",
    "                       'span',{'class':'style-scope yt-formatted-string'}],\n",
    "            'duration_s':\"return document.getElementById('movie_player').getDuration()\",\n",
    "            'channel_name':['div',{'class':'style-scope ytd-video-owner-renderer', 'id':'upload-info'},'yt-formatted-string'],\n",
    "            'subs':['yt-formatted-string',{'id':'owner-sub-count'}],\n",
    "            'link_top_re':['a',{'id':'thumbnail','class':'yt-simple-endpoint inline-block style-scope ytd-thumbnail'}]\n",
    "           }\n",
    "\n",
    "def create_xpath():\n",
    "    # Tạo dictionary chứa các xpath\n",
    "    return {\n",
    "            'title':'//*[@id=\"container\"]/h1/yt-formatted-string',\n",
    "            'date_public':'//*[@id=\"date\"]/yt-formatted-string',\n",
    "            'description':'//*[@id=\"description\"]',\n",
    "            'views':'//*[@id=\"count\"]/ytd-video-view-count-renderer/span[1]',\n",
    "            'likes':'//*[@id=\"text\"]',\n",
    "            'dislikes':'//*[@id=\"text\"]',\n",
    "            'num_cmt':'//*[@id=\"count\"]/yt-formatted-string/span[1]',\n",
    "            'channel_name':'//*[@id=\"text\"]/a',\n",
    "            'subs':'//*[@id=\"owner-sub-count\"]',\n",
    "            'link_top_re':'//*[@id=\"thumbnail\"]'\n",
    "           }\n",
    "\n",
    "def create_others():\n",
    "    # Tạo dictionary chứa các thông tin quan trọng khác\n",
    "    return {\n",
    "            'search_link':'https://www.youtube.com/results?search_query=',\n",
    "            'translate_symbols':{'`':'%60','@':'%40','#':'%23','$':'%24',\n",
    "                                 '%':'%25','^':'%5E','&':'%26','+':'%2B',\n",
    "                                 '=':'%3D','|':'%7C','\\\\':'%5C','}':'%7D',\n",
    "                                 ']':'%5D','{':'%7B','[':'%5B','\\'':'%27',\n",
    "                                 ':':'%3A',';':'%3B','?':'%3F','/':'%2F',\n",
    "                                 ',':'%2C',' ':'+'},\n",
    "            'PATH':'C:\\Program Files (x86)\\chromedriver.exe',\n",
    "            'youtube_url':'https://www.youtube.com',\n",
    "            'video10_xpath':'/html/body/ytd-app/div/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-video-renderer[10]/div[1]/div/div[1]/div/h3/a/yt-formatted-string',\n",
    "            'path_df_scrape':'D:\\python_proj\\scrape_youtube\\scrape_top_videos\\df_scrape\\\\',\n",
    "            'path_error_urls':'D:\\python_proj\\scrape_youtube\\scrape_top_videos\\error_urls\\\\',\n",
    "            'path_log_tracking':'D:\\python_proj\\scrape_youtube\\scrape_top_videos\\log_tracking\\\\',\n",
    "\n",
    "           }\n",
    "\n",
    "def update_path(start_time, path):\n",
    "    year = 'y_'+start_time.strftime('%Y')\n",
    "    month = 'm_'+start_time.strftime('%m')\n",
    "    day = 'd_'+start_time.strftime('%d')\n",
    "    if year not in os.listdir(path):\n",
    "        os.mkdir(path+year+'\\\\')\n",
    "    path += year + '\\\\'\n",
    "    if month not in os.listdir(path):\n",
    "        os.mkdir(path+month+'\\\\')\n",
    "    path += month + '\\\\'\n",
    "    if day not in os.listdir(path):\n",
    "        os.mkdir(path+day+'\\\\')\n",
    "    path += day + '\\\\'\n",
    "    return path\n",
    "\n",
    "def errors():\n",
    "    return{\n",
    "        'fatalError':'!! Exceed the max_error !!\\n\\t=> Terminate!',\n",
    "        '1stError':'Error in creating DataFrame df_scrape or in assert',\n",
    "        'main_driver_fail':'! Can not access to search youtube web !',\n",
    "        '2ndError':'Error in setting driver, or in BeautifulSoup driver lxml video_urls\\n\\t=> Try to redo',\n",
    "        'congesting':'May be congested network or driver\\n\\t=> Try to redo',\n",
    "        'all_driver_fail':'Can not access any link of the list video_urls',\n",
    "        'driver_fail':'! Can not access to video, or soup !\\n\\t=> Switch to another url',\n",
    "        '3rdError':'Error in driver.get, or not being able to soup, or error in set driver, or in creating csv, txt\\n\\t=> Try to redo',\n",
    "        '4thError':'Error in  driver.get, or in concating to dataframe df_scrape, or not being ablle to soup for certain video\\n\\t=> Try to redo',\n",
    "        '1stErrorRe':'Fail for loop (re)',\n",
    "        '2ndErrorRe':'Error in setting driver, or in BeautifulSoup driver lxml video_urls (re)\\n\\t=> Try to redo',\n",
    "        'driver_fail_re':'! Can not access to video, or soup, or record to dataframe (re) !\\n\\t=> Switch to another url',\n",
    "        'data_loc':'Fail to record data (re)',\n",
    "    }\n",
    "\n",
    "def wait_loading_web(wait, list_xpath):\n",
    "    for i in list_xpath:\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, i)))\n",
    "    return\n",
    "\n",
    "# def sys_error():\n",
    "#     return '\\n'.join(['Traceback:'] + list(map(lambda x: '\\t'+str(x).strip('\\n'), list(sys.exc_info()))))+'\\n'\n",
    "\n",
    "def traceback_error():\n",
    "    return '='*164+'\\n'+traceback.format_exc().strip('\\n')+'\\n'+'='*164+'\\n'\n",
    "\n",
    "def force_kill_driver(driver, path_log_detail, start_time):\n",
    "    try:\n",
    "        os.system('taskkill /f /im chrome.exe')\n",
    "        driver.quit()\n",
    "    except Exception as err:\n",
    "        posible_bugs = 'Can not kill or quit driver'\n",
    "        print(posible_bugs)\n",
    "        print(err)\n",
    "        traceback.print_exc()\n",
    "        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "            f.write(posible_bugs+'\\n')\n",
    "            f.write(traceback_error())\n",
    "    return\n",
    "\n",
    "def force_build_driver(path_log_detail, start_time):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(create_others()['PATH'])\n",
    "    except Exception as err:\n",
    "        posible_bugs = 'Can not build driver'\n",
    "        print(posible_bugs)\n",
    "        print(err)\n",
    "        traceback.print_exc()\n",
    "        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "            f.write(posible_bugs+'\\n')\n",
    "            f.write(traceback_error())\n",
    "    return driver\n",
    "\n",
    "def scrape_top_videos(key_search='', top_videos = 10, limit_num_link_re = 10,\n",
    "    repeat_driver_congestion = 3, max_error = 10, limited_time_load_web = 13):\n",
    "    # Nhập từ khóa, function tự động điền lên youtube và thu thập thông tin top video\n",
    "    # (đọc thông tin trong function create_df_scrape())\n",
    "    try: # 1st error\n",
    "        driver = ''\n",
    "        num_error = 0\n",
    "        fatal_error = 0\n",
    "        start_time = datetime.now()\n",
    "        path_df_detail = update_path(start_time, create_others()['path_df_scrape'])\n",
    "        path_err_detail = update_path(start_time, create_others()['path_error_urls'])\n",
    "        path_log_detail = update_path(start_time, create_others()['path_log_tracking'])\n",
    "        assert (top_videos >= 1 and top_videos <= 10), 'Minimum 1 video, maximum 10 videos, or adjust video10_xpath and set Keys.PAGE_DOWN if want to get more videos urls'\n",
    "        assert (limit_num_link_re >= 1 and limit_num_link_re <= 10), 'Minimum 1 recommended video, maximum 10 recommended videos, or adjust link_top_re and set Keys.PAGE_DOWN if want to get more recommended videos urls'\n",
    "        assert (repeat_driver_congestion >= 1), 'Minimum 1 repreat, it would be counted as the first visit to the website'\n",
    "        assert (max_error >= 1), 'Minimum 1 repreat, it would be assumed that no errors are allowed when running the program'\n",
    "        assert (repeat_driver_congestion <= max_error), 'repeat_driver_congestion must be less than or equal to max_error'\n",
    "        assert (limited_time_load_web >= 5), 'Minimum 5 seconds, it is the time allowed to wait for the web page to load'\n",
    "        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "            f.write('Start\\n')\n",
    "        df_scrape = create_df_scrape()\n",
    "        driver = ''\n",
    "        tit_des_tags_re = ''\n",
    "        if key_search == '':\n",
    "            key_search=input('Search: ')\n",
    "        while key_search == '':\n",
    "            key_search=input('Search: ')\n",
    "        yt_word=''\n",
    "        for word_char in key_search:\n",
    "            if word_char in create_others()['translate_symbols'].keys():\n",
    "                yt_word += create_others()['translate_symbols'][word_char]\n",
    "            else:\n",
    "                yt_word += word_char\n",
    "        main_driver_success = 0\n",
    "        main_driver_repeat = 0\n",
    "        done = 0\n",
    "        while not (main_driver_success == 1 or main_driver_repeat >= repeat_driver_congestion):\n",
    "            try: # 2nd error\n",
    "                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                    f.write('while I\\n')\n",
    "                driver = force_build_driver(path_log_detail, start_time)\n",
    "                func_timeout(limited_time_load_web,driver.get,args=(create_others()['search_link']+yt_word,))\n",
    "                html = driver.find_element_by_tag_name('html')\n",
    "                for i in range(1):\n",
    "                    html.send_keys(Keys.PAGE_DOWN)\n",
    "                wait = WebDriverWait(driver, limited_time_load_web)\n",
    "                wait.until(EC.presence_of_element_located((By.XPATH, create_others()['video10_xpath'])))\n",
    "                video_urls = list(map(lambda x: x.get('href'),\n",
    "                                        list(BeautifulSoup(driver.page_source.encode('utf-8').strip(), 'lxml').\\\n",
    "                                            findAll('a', id='video-title'))))\n",
    "                print(len(video_urls))\n",
    "                if len(video_urls) >= top_videos:\n",
    "                    print('\\n\\t=> Num. urls: Ok.')\n",
    "                    video_urls = video_urls[:top_videos]\n",
    "                else:\n",
    "                    print('\\t=> Num. urls: Not enough!')\n",
    "                main_driver_success = 1\n",
    "                num_error = 0\n",
    "                id_video_url = 0\n",
    "                while not (id_video_url >= len(video_urls) or done == 1 or num_error >= max_error):\n",
    "                    with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                        f.write('while I.I\\n')\n",
    "                    driver_success = 0\n",
    "                    driver_repeat = 0\n",
    "                    while not (driver_success == 1 or driver_repeat >= repeat_driver_congestion or num_error >= max_error):\n",
    "                        try: # 3rd error\n",
    "                            func_timeout(limited_time_load_web,driver.get,args=(create_others()['youtube_url']+video_urls[id_video_url],))\n",
    "                            html = driver.find_element_by_tag_name('html')\n",
    "                            for i in range(1):\n",
    "                                html.send_keys(Keys.PAGE_DOWN)\n",
    "                            wait = WebDriverWait(driver, limited_time_load_web)\n",
    "                            func_timeout(limited_time_load_web, wait_loading_web, args=(wait, create_xpath().values(),))\n",
    "                            soup = BeautifulSoup(driver.page_source.encode('utf-8').strip(), 'lxml')\n",
    "                            try:\n",
    "                                duration_s = driver.execute_script(create_dic_find_key()['duration_s'])\n",
    "                            except:\n",
    "                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                    f.write('Fail to find duration\\n')\n",
    "                                    f.write(traceback_error())\n",
    "                                duration_s = np.NaN\n",
    "                                traceback.print_exc()\n",
    "                            driver_success = 1\n",
    "                            num_error = 0\n",
    "                            soup_update = 1\n",
    "                            id_video_url_new = 0\n",
    "                            video_urls_new = video_urls[id_video_url+1:]\n",
    "                            while not (id_video_url_new >= (len(video_urls_new)+1) or num_error >= max_error):\n",
    "                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                    f.write('while I.I.I\\n')\n",
    "                                driver_2_success = 0\n",
    "                                driver_2_repeat = 0\n",
    "                                while not (driver_2_success == 1 or driver_2_repeat >= repeat_driver_congestion or num_error >= max_error):\n",
    "                                    try: # 4th error\n",
    "                                        if id_video_url_new < (len(video_urls_new)):\n",
    "                                            func_timeout(limited_time_load_web,driver.get,args=(create_others()['youtube_url']+video_urls_new[id_video_url_new],))\n",
    "                                            html = driver.find_element_by_tag_name('html')\n",
    "                                            for i in range(1):\n",
    "                                                html.send_keys(Keys.PAGE_DOWN)\n",
    "                                        else:\n",
    "                                            driver_2_success = 1\n",
    "                                        if soup_update == 1:\n",
    "                                            # -------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                title, date_public = list(map(lambda x: x.text,\n",
    "                                                                                list(soup.find_all(create_dic_find_key()['title_date_public'][0],\n",
    "                                                                                                    attrs=create_dic_find_key()['title_date_public'][1]))))\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find title and date_public\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                title = np.NaN\n",
    "                                                date_public = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # -------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                description = ' '.join(soup.find_all(create_dic_find_key()['description'][0],\n",
    "                                                                                attrs=create_dic_find_key()['description'][1])\\\n",
    "                                                                        [0].find('span').text.replace('\\n',' ').replace('\\t',' ').split(' ')[:100])\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find description\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                description = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # -------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                tags = soup.find_all(create_dic_find_key()['tags'][0],attrs=create_dic_find_key()['tags'][1])[0]['content']\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find tags\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                tags = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # -------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                views = int(soup.find(create_dic_find_key()['views'][0], attrs=create_dic_find_key()['views'][1])\\\n",
    "                                                            .text.strip(' views').replace(',','').replace('No','0'))\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find views\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                views = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # -------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                likes, dislikes = list(map(lambda x: int(x[create_dic_find_key()['likes_dislikes'][2]].strip(' dislikes').replace(',','').replace('No','0')),\n",
    "                                                                            list(soup.find_all(create_dic_find_key()['likes_dislikes'][0],\n",
    "                                                                                                attrs=create_dic_find_key()['likes_dislikes'][1]))))\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find likes and dislikes\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                likes = np.NaN\n",
    "                                                dislikes = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # -------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                num_cmt = int(soup.find(create_dic_find_key()['num_cmt'][0],attrs=create_dic_find_key()['num_cmt'][1]).\\\n",
    "                                                    find(create_dic_find_key()['num_cmt'][2],attrs=create_dic_find_key()['num_cmt'][3]).text.replace(',',''))\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find number of comments\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                channel_name = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # -------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                channel_name = soup.find(create_dic_find_key()['channel_name'][0],attrs=create_dic_find_key()['channel_name'][1]).\\\n",
    "                                                                find(create_dic_find_key()['channel_name'][2]).text\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find channel_name\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                channel_name = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # -------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                subs = soup.find(create_dic_find_key()['subs'][0],attrs=create_dic_find_key()['subs'][1]).text.strip(' subscribers')\n",
    "                                                if subs[-1].isdigit():\n",
    "                                                    subs = int(subs)\n",
    "                                                else:\n",
    "                                                    subs = int(float(subs[:-1])*{'K':1000, 'M':1000000, 'B':1000000000}[subs[-1]])\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find number of subscriptions\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                subs = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # ------------------------------------------------------------\n",
    "                                            try:\n",
    "                                                link_top_re = []\n",
    "                                                top = 0\n",
    "                                                for i in soup.find_all(create_dic_find_key()['link_top_re'][0],attrs=create_dic_find_key()['link_top_re'][1]):\n",
    "                                                    try:\n",
    "                                                        if 'list' not in i['href']:\n",
    "                                                            link_top_re += [i['href']]\n",
    "                                                            top += 1\n",
    "                                                            if top == limit_num_link_re:\n",
    "                                                                break\n",
    "                                                    except:\n",
    "                                                        pass\n",
    "                                                print('Len of link_top_re: '+str(len(link_top_re)))\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Len of link_top_re: '+str(len(link_top_re))+'\\n')\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find link_top_re\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                link_top_re = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            # ------------------------------------------------------------\n",
    "                                            df_scrape = pd.concat([df_scrape,pd.DataFrame([[key_search, id_video_url+id_video_url_new+1, title, date_public, description, tags, views,\n",
    "                                                likes, dislikes, num_cmt, duration_s, create_others()['youtube_url']+video_urls[id_video_url+id_video_url_new],\n",
    "                                                channel_name, subs, link_top_re, tit_des_tags_re]],\n",
    "                                                columns = df_scrape.columns)], axis=0, ignore_index=True)\n",
    "                                            with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                f.write('Recorded data\\n')\n",
    "                                            soup_update = 0\n",
    "                                        # -------------------------------------------------------------\n",
    "                                        if id_video_url_new < (len(video_urls_new)):\n",
    "                                            wait = WebDriverWait(driver, limited_time_load_web)\n",
    "                                            func_timeout(limited_time_load_web, wait_loading_web, args=(wait, create_xpath().values(),))\n",
    "                                            try:\n",
    "                                                duration_s = driver.execute_script(create_dic_find_key()['duration_s'])\n",
    "                                            except:\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('Fail to find duration\\n')\n",
    "                                                    f.write(traceback_error())\n",
    "                                                duration_s = np.NaN\n",
    "                                                traceback.print_exc()\n",
    "                                            soup = BeautifulSoup(driver.page_source.encode('utf-8').strip(), 'lxml')\n",
    "                                            driver_2_success = 1\n",
    "                                            num_error = 0\n",
    "                                            soup_update = 1\n",
    "                                        # -------------------------------------------------------------\n",
    "                                    except FunctionTimedOut as err:\n",
    "                                        print(errors()['congesting'])\n",
    "                                        print(err)\n",
    "                                        traceback.print_exc()\n",
    "                                        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                            f.write(errors()['congesting']+'\\n')\n",
    "                                            f.write(traceback_error())\n",
    "                                        driver_2_success = 0\n",
    "                                        driver_2_repeat += 1\n",
    "                                        num_error += 1\n",
    "                                        force_kill_driver(driver, path_log_detail, start_time)\n",
    "                                        driver = force_build_driver(path_log_detail, start_time)\n",
    "                                    except Exception as err: # 4th error\n",
    "                                        print(errors()['4thError'])\n",
    "                                        print(err)\n",
    "                                        traceback.print_exc()\n",
    "                                        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                            f.write(errors()['4thError']+'\\n')\n",
    "                                            f.write(traceback_error())\n",
    "                                        driver_2_success = 0\n",
    "                                        driver_2_repeat += 1\n",
    "                                        num_error += 1\n",
    "                                        force_kill_driver(driver, path_log_detail, start_time)\n",
    "                                        driver = force_build_driver(path_log_detail, start_time)\n",
    "                                if num_error < max_error and driver_2_success == 0:\n",
    "                                    print(errors()['driver_fail'])\n",
    "                                    with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                        f.write(errors()['driver_fail']+'\\n')\n",
    "                                    with open(path_err_detail+'err_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                        f.write(video_urls_new[id_video_url_new]+'\\n')\n",
    "                                id_video_url_new += 1\n",
    "                            # =============================================================================\n",
    "                            try:\n",
    "                                ind_ = 0\n",
    "                                num_rows = df_scrape.shape[0]\n",
    "                                while not (ind_ >= num_rows or num_error >= max_error):\n",
    "                                    link_top_re_data = df_scrape.loc[ind_, 'link_top_re']\n",
    "                                    tit_des_tags_re = ''\n",
    "                                    id_link_re = 0\n",
    "                                    if isinstance(link_top_re_data, list):\n",
    "                                        while not (id_link_re >= len(link_top_re_data) or num_error >= max_error):\n",
    "                                            with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                f.write('while I.I.I.I\\n')\n",
    "                                            driver_success_re = 0\n",
    "                                            driver_repeat_re = 0\n",
    "                                            while not (driver_success_re == 1 or driver_repeat_re >= repeat_driver_congestion or num_error >= max_error):\n",
    "                                                try:\n",
    "                                                    func_timeout(limited_time_load_web,driver.get,args=(create_others()['youtube_url']+link_top_re_data[id_link_re],))\n",
    "                                                    html = driver.find_element_by_tag_name('html')\n",
    "                                                    for i in range(1):\n",
    "                                                        html.send_keys(Keys.PAGE_DOWN)\n",
    "                                                    list_wait_element_re = [create_xpath()['title'],create_xpath()['description']]\n",
    "                                                    wait = WebDriverWait(driver, limited_time_load_web)\n",
    "                                                    func_timeout(limited_time_load_web, wait_loading_web, args=(wait, list_wait_element_re,))\n",
    "                                                    soup = BeautifulSoup(driver.page_source.encode('utf-8').strip(), 'lxml')\n",
    "                                                    driver_success_re = 1\n",
    "                                                    # -------------------------------------------------------------\n",
    "                                                    try:\n",
    "                                                        title, date_public = list(map(lambda x: x.text,\n",
    "                                                                                        list(soup.find_all(create_dic_find_key()['title_date_public'][0],\n",
    "                                                                                                            attrs=create_dic_find_key()['title_date_public'][1]))))\n",
    "                                                    except:\n",
    "                                                        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                            f.write('\\tFail to find title and date_public (re)\\n')\n",
    "                                                            f.write(traceback_error())\n",
    "                                                        title = np.NaN\n",
    "                                                        date_public = np.NaN\n",
    "                                                        traceback.print_exc()\n",
    "                                                    # -------------------------------------------------------------\n",
    "                                                    try:\n",
    "                                                        description = ' '.join(soup.find_all(create_dic_find_key()['description'][0],\n",
    "                                                                                        attrs=create_dic_find_key()['description'][1])\\\n",
    "                                                                                [0].find('span').text.replace('\\n',' ').replace('\\t',' ').split(' ')[:100])\n",
    "                                                    except:\n",
    "                                                        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                            f.write('\\tFail to find description (re)\\n')\n",
    "                                                            f.write(traceback_error())\n",
    "                                                        description = np.NaN\n",
    "                                                        traceback.print_exc()\n",
    "                                                    # -------------------------------------------------------------\n",
    "                                                    try:\n",
    "                                                        tags = soup.find_all(create_dic_find_key()['tags'][0],attrs=create_dic_find_key()['tags'][1])[0]['content']\n",
    "                                                    except:\n",
    "                                                        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                            f.write('\\tFail to find tags (re)\\n')\n",
    "                                                            f.write(traceback_error())\n",
    "                                                        tags = np.NaN\n",
    "                                                        traceback.print_exc()\n",
    "                                                    # -------------------------------------------------------------\n",
    "                                                    tit_des_tags_re += '{}\\n{}\\n{}\\n\\n'.format(title, description, tags)\n",
    "                                                    num_error = 0\n",
    "                                                    id_link_re += 1\n",
    "                                                except FunctionTimedOut as err:\n",
    "                                                    print(errors()['congesting'])\n",
    "                                                    print(err)\n",
    "                                                    traceback.print_exc()\n",
    "                                                    with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                        f.write('\\t'+errors()['congesting']+'(re)'+'\\n')\n",
    "                                                        f.write(traceback_error())\n",
    "                                                    driver_success_re = 0\n",
    "                                                    driver_repeat_re += 1\n",
    "                                                    num_error += 1\n",
    "                                                    force_kill_driver(driver, path_log_detail, start_time)\n",
    "                                                    driver = force_build_driver(path_log_detail, start_time)\n",
    "                                                except Exception as err:\n",
    "                                                    print(errors()['2ndErrorRe'])\n",
    "                                                    print(err)\n",
    "                                                    traceback.print_exc()\n",
    "                                                    with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                        f.write('\\t'+errors()['2ndErrorRe']+'\\n')\n",
    "                                                        f.write(traceback_error())\n",
    "                                                    driver_success_re = 0\n",
    "                                                    driver_repeat_re += 1\n",
    "                                                    num_error += 1\n",
    "                                                    force_kill_driver(driver, path_log_detail, start_time)\n",
    "                                                    driver = force_build_driver(path_log_detail, start_time)\n",
    "                                            if num_error < max_error and driver_success_re == 0:\n",
    "                                                print(errors()['driver_fail_re'])\n",
    "                                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('\\t'+errors()['driver_fail_re']+'\\n')\n",
    "                                                with open(path_err_detail+'err_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                    f.write('\\t'+link_top_re_data[id_link_re]+'\\n')\n",
    "                                                id_link_re += 1\n",
    "                                        try:\n",
    "                                            df_scrape.loc[ind_,'tit_des_tags_re'] = tit_des_tags_re\n",
    "                                            print('Loc successful (re)')\n",
    "                                            with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                f.write('\\t'+'Loc successful (re)'+'\\n')\n",
    "                                        except:\n",
    "                                            print(errors()['data_loc'])\n",
    "                                            with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                                f.write('\\t'+errors()['data_loc']+'\\n')\n",
    "                                    ind_ += 1\n",
    "                            except Exception as err:\n",
    "                                print(errors()['1stErrorRe'])\n",
    "                                print(err)\n",
    "                                traceback.print_exc()\n",
    "                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                    f.write(errors()['1stErrorRe']+'\\n')\n",
    "                                    f.write(traceback_error())\n",
    "                            # =============================================================================\n",
    "                            if df_scrape.notnull().any().any():\n",
    "                                df_scrape.set_index(df_scrape.columns[0]).to_csv(path_df_detail+'df_'+start_time.strftime('%Hh_%Mm_%Ss')+'.csv')\n",
    "                                success_mess = 'Successfully save data to file csv.'\n",
    "                                print(success_mess)\n",
    "                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                    f.write(success_mess + '\\n')\n",
    "                            else:\n",
    "                                fail_mess = 'Fail to scrape data, data is empty.'\n",
    "                                print(fail_mess)\n",
    "                                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                    f.write(fail_mess + '\\n')\n",
    "                            done = 1\n",
    "                        except FunctionTimedOut as err:\n",
    "                            print(errors()['congesting'])\n",
    "                            print(err)\n",
    "                            traceback.print_exc()\n",
    "                            with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                f.write(errors()['congesting']+'\\n')\n",
    "                                f.write(traceback_error())\n",
    "                            driver_success = 0\n",
    "                            driver_repeat += 1\n",
    "                            num_error += 1\n",
    "                            force_kill_driver(driver, path_log_detail, start_time)\n",
    "                            driver = force_build_driver(path_log_detail, start_time)\n",
    "                        except Exception as err: # 3rd error\n",
    "                            print(errors()['3rdError'])\n",
    "                            print(err)\n",
    "                            traceback.print_exc()\n",
    "                            with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                                f.write(errors()['3rdError']+'\\n')\n",
    "                                f.write(traceback_error())\n",
    "                            driver_success = 0\n",
    "                            driver_repeat += 1\n",
    "                            num_error += 1\n",
    "                            force_kill_driver(driver, path_log_detail, start_time)\n",
    "                            driver = force_build_driver(path_log_detail, start_time)\n",
    "                    if num_error < max_error and driver_success == 0:\n",
    "                        print(errors()['driver_fail'])\n",
    "                        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                            f.write(errors()['driver_fail']+'\\n')\n",
    "                        with open(path_err_detail+'err_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                            f.write(video_urls[id_video_url]+'\\n')\n",
    "                    id_video_url += 1\n",
    "                if num_error < max_error and driver_success == 0:\n",
    "                    print(errors()['all_driver_fail'])\n",
    "                    with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                        f.write(errors()['all_driver_fail']+'\\n')\n",
    "            except FunctionTimedOut as err:\n",
    "                print(errors()['congesting'])\n",
    "                print(err)\n",
    "                traceback.print_exc()\n",
    "                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                    f.write(errors()['congesting']+'\\n')\n",
    "                    f.write(traceback_error())\n",
    "                main_driver_success = 0\n",
    "                main_driver_repeat += 1\n",
    "                num_error += 1\n",
    "                force_kill_driver(driver, path_log_detail, start_time)\n",
    "            except Exception as err: #2nd error\n",
    "                print(errors()['2ndError'])\n",
    "                print(err)\n",
    "                traceback.print_exc()\n",
    "                with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                    f.write(errors()['2ndError']+'\\n')\n",
    "                    f.write(traceback_error())\n",
    "                main_driver_success = 0\n",
    "                main_driver_repeat += 1\n",
    "                num_error += 1\n",
    "                force_kill_driver(driver, path_log_detail, start_time)\n",
    "        if main_driver_success == 0:\n",
    "            print(errors()['main_driver_fail'])\n",
    "            with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                f.write(errors()['main_driver_fail']+'\\n')\n",
    "    except Exception as err: # 1st error\n",
    "        print(errors()['1stError'])\n",
    "        print(err)\n",
    "        traceback.print_exc()\n",
    "        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "            f.write(errors()['1stError']+'\\n')\n",
    "            f.write(traceback_error())\n",
    "    force_kill_driver(driver, path_log_detail, start_time)\n",
    "    try:\n",
    "        if num_error >= max_error:\n",
    "            fatal_error = 1\n",
    "            print(errors()['fatalError'])\n",
    "            with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "                f.write(errors()['fatalError']+'\\n')\n",
    "        with open(path_log_detail+'log_'+start_time.strftime('%Hh_%Mm_%Ss')+'.txt','a+') as f:\n",
    "            f.write('Total execution time:\\n\\t'+str(datetime.now() - start_time)+'\\nFinish.\\n')\n",
    "        print('Finish.')\n",
    "    except Exception as err:\n",
    "        print('!! Can not run the end part !!\\n\\tTerminate!')\n",
    "        print(err)\n",
    "        traceback.print_exc()\n",
    "    return fatal_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fatal_error = scrape_top_videos(key_search='', top_videos = 10, limit_num_link_re = 10,\n",
    "    repeat_driver_congestion = 3, max_error = 10, limited_time_load_web = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
